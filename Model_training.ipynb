{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "df = pd.read_csv('dataset.csv', engine='python', encoding='Utf-8')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['depressed'], test_size=0.2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweets'] = df['tweet'].apply(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = '''\n",
    "de \n",
    "a \n",
    "o \n",
    "que \n",
    "e \n",
    "do \n",
    "da \n",
    "em \n",
    "um \n",
    "para \n",
    "é \n",
    "com \n",
    "não \n",
    "uma \n",
    "os \n",
    "no \n",
    "se \n",
    "na \n",
    "por \n",
    "mais \n",
    "as \n",
    "dos \n",
    "como \n",
    "mas \n",
    "foi \n",
    "ao \n",
    "ele \n",
    "das \n",
    "tem \n",
    "à \n",
    "seu \n",
    "sua \n",
    "ou \n",
    "ser \n",
    "quando \n",
    "muito \n",
    "há \n",
    "nos \n",
    "já \n",
    "está \n",
    "também \n",
    "só \n",
    "pelo \n",
    "pela \n",
    "até \n",
    "isso \n",
    "ela \n",
    "entre \n",
    "era \n",
    "depois \n",
    "sem \n",
    "mesmo \n",
    "aos \n",
    "ter \n",
    "seus \n",
    "quem \n",
    "nas \n",
    "me \n",
    "esse \n",
    "eles \n",
    "estão \n",
    "você \n",
    "tinha \n",
    "foram \n",
    "essa \n",
    "num \n",
    "nem \n",
    "suas \n",
    "meu \n",
    "às \n",
    "minha \n",
    "têm \n",
    "numa \n",
    "pelos \n",
    "elas \n",
    "havia \n",
    "seja \n",
    "qual \n",
    "será \n",
    "nós \n",
    "tenho \n",
    "lhe \n",
    "deles \n",
    "essas \n",
    "esses \n",
    "pelas \n",
    "este \n",
    "fosse \n",
    "dele \n",
    "tu \n",
    "te \n",
    "vocês \n",
    "vos \n",
    "lhes \n",
    "meus \n",
    "minhas\n",
    "teu \n",
    "tua\n",
    "teus\n",
    "tuas\n",
    "nosso \n",
    "nossa\n",
    "nossos\n",
    "nossas\n",
    "dela \n",
    "delas \n",
    "esta \n",
    "estes \n",
    "estas \n",
    "aquele \n",
    "aquela \n",
    "aqueles \n",
    "aquelas \n",
    "isto \n",
    "aquilo \n",
    "estou\n",
    "está\n",
    "estamos\n",
    "estão\n",
    "estive\n",
    "esteve\n",
    "estivemos\n",
    "estiveram\n",
    "estava\n",
    "estávamos\n",
    "estavam\n",
    "estivera\n",
    "estivéramos\n",
    "esteja\n",
    "estejamos\n",
    "estejam\n",
    "estivesse\n",
    "estivéssemos\n",
    "estivessem\n",
    "estiver\n",
    "estivermos\n",
    "estiverem\n",
    "hei\n",
    "há\n",
    "havemos\n",
    "hão\n",
    "houve\n",
    "houvemos\n",
    "houveram\n",
    "houvera\n",
    "houvéramos\n",
    "haja\n",
    "hajamos\n",
    "hajam\n",
    "houvesse\n",
    "houvéssemos\n",
    "houvessem\n",
    "houver\n",
    "houvermos\n",
    "houverem\n",
    "houverei\n",
    "houverá\n",
    "houveremos\n",
    "houverão\n",
    "houveria\n",
    "houveríamos\n",
    "houveriam\n",
    "sou\n",
    "somos\n",
    "são\n",
    "era\n",
    "éramos\n",
    "eram\n",
    "fui\n",
    "foi\n",
    "fomos\n",
    "foram\n",
    "fora\n",
    "fôramos\n",
    "seja\n",
    "sejamos\n",
    "sejam\n",
    "fosse\n",
    "fôssemos\n",
    "fossem\n",
    "for\n",
    "formos\n",
    "forem\n",
    "serei\n",
    "será\n",
    "seremos\n",
    "serão\n",
    "seria\n",
    "seríamos\n",
    "seriam\n",
    "tenho\n",
    "tem\n",
    "temos\n",
    "tém\n",
    "tinha\n",
    "tínhamos\n",
    "tinham\n",
    "tive\n",
    "teve\n",
    "tivemos\n",
    "tiveram\n",
    "tivera\n",
    "tivéramos\n",
    "tenha\n",
    "tenhamos\n",
    "tenham\n",
    "tivesse\n",
    "tivéssemos\n",
    "tivessem\n",
    "tiver\n",
    "tivermos\n",
    "tiverem\n",
    "terei\n",
    "terá\n",
    "teremos\n",
    "terão\n",
    "teria\n",
    "teríamos\n",
    "teriam\n",
    "pra\n",
    "O\n",
    "q\n",
    "https\n",
    "http\n",
    "A\n",
    "tá\n",
    "pq\n",
    "pic\n",
    "E\n",
    "'''\n",
    "stopwords = re.split(r'\\W', stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "review_lines = list()\n",
    "lines = df['tweet'].values.tolist()\n",
    "\n",
    "for line in lines:   \n",
    "    tokens = word_tokenize(str(line))\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word    \n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words    \n",
    "    words = [w for w in words if not w in stopwords]\n",
    "    review_lines.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(review_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "# train word2vec model\n",
    "model = gensim.models.Word2Vec(sentences=review_lines, size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n",
    "# vocab size\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'tweets_embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('mal')#, topn =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'tweets_embedding_word2vec.txt'),  encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['tweet'],df['depressed'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reviews = X_train + X_test\n",
    "max_length = 100 # try other options like mean of sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# vectorize the text samples into a 2D integer tensor\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(review_lines)\n",
    "sequences = tokenizer_obj.texts_to_sequences(review_lines)\n",
    "\n",
    "# pad sequences\n",
    "word_index = tokenizer_obj.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "review_pad = pad_sequences(sequences, maxlen=max_length)\n",
    "sentiment =  df['depressed'].values\n",
    "print('Shape of review tensor:', review_pad.shape)\n",
    "print('Shape of sentiment tensor:', sentiment.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(review_pad.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "review_pad = review_pad[indices]\n",
    "sentiment = sentiment[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * review_pad.shape[0])\n",
    "\n",
    "X_train_pad = review_pad[:-num_validation_samples]\n",
    "y_train = sentiment[:-num_validation_samples]\n",
    "X_test_pad = review_pad[-num_validation_samples:]\n",
    "y_test = sentiment[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of X_train_pad tensor:', X_train_pad.shape)\n",
    "print('Shape of y_train tensor:', y_train.shape)\n",
    "\n",
    "print('Shape of X_test_pad tensor:', X_test_pad.shape)\n",
    "print('Shape of y_test tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM =100\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.initializers import Constant\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=max_length,\n",
    "                            trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 128, activation = 'relu'))\n",
    "model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train_pad, y_train, batch_size=128, epochs=20, validation_data=(X_test_pad, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
